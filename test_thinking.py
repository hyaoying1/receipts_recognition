import os
import base64
import json
import time
from datetime import datetime
from pathlib import Path
import re
from concurrent.futures import ThreadPoolExecutor, as_completed

import fitz  # PyMuPDF, ç”¨äº PDF è½¬å›¾ç‰‡
import matplotlib.pyplot as plt
from openai import OpenAI


# =========================
# åŸºæœ¬é…ç½®
# =========================
BASE_URL = "https://dashscope.aliyuncs.com/compatible-mode/v1"
API_KEY_ENV = "OPENAI_API_KEY"

DATA_DIR = "./data"
OUTPUT_DIR = Path("outputs")
OUTPUT_JSON = OUTPUT_DIR / "qwen_vl_benchmark_results.json"
OUTPUT_PLOT = OUTPUT_DIR / "avg_time_plot.png"
PROMPT_FILE = "prompt.txt"

# å¹¶è¡Œçº¿ç¨‹æ•°
MAX_WORKERS = 8

# æ¨¡å‹åˆ—è¡¨
MODELS = [
    "qwen-vl-plus",
    "qwen-vl-max",
    "qwen2.5-vl-3b-instruct",
    "qwen2.5-vl-7b-instruct",
    "qwen2.5-vl-32b-instruct",
    "qwen2.5-vl-72b-instruct",
    "qwen3-vl-8b-instruct",
    "qwen3-vl-30b-a3b-instruct",
]



# =========================
# å·¥å…·å‡½æ•°
# =========================
def now_iso() -> str:
    """è¿”å›å½“å‰ UTC æ—¶é—´çš„ ISO æ ¼å¼å­—ç¬¦ä¸²"""
    return datetime.utcnow().isoformat() + "Z"


def load_prompt() -> str:
    """è¯»å– prompt.txt"""
    path = Path(PROMPT_FILE)
    if not path.exists():
        raise FileNotFoundError(f"æœªæ‰¾åˆ° {PROMPT_FILE}ï¼Œè¯·åœ¨è„šæœ¬åŒçº§ç›®å½•ä¸‹åˆ›å»ºè¯¥æ–‡ä»¶ã€‚")
    return path.read_text(encoding="utf-8")


def iter_docs(root_dir: str):
    """
    éå† ./data ä¸‹çš„æ–‡ä»¶ï¼Œåªä¿ç•™ jpg/jpeg/png/pdf
    è¿”å› Path å¯¹è±¡
    """
    root = Path(root_dir)
    if not root.exists():
        raise FileNotFoundError(f"æ•°æ®ç›®å½•ä¸å­˜åœ¨: {root_dir}")

    for p in sorted(root.iterdir()):
        if not p.is_file():
            continue
        if p.suffix.lower() in (".jpg", ".jpeg", ".png", ".pdf"):
            yield p


def make_data_url_from_bytes(data: bytes, mime: str) -> str:
    b64 = base64.b64encode(data).decode("utf-8")
    return f"data:{mime};base64,{b64}"


def encode_image_file_to_data_url(path: Path) -> str:
    """
    é’ˆå¯¹ jpg/jpeg/png ç›´æ¥è¯»å–æ–‡ä»¶å¹¶è½¬ä¸º data URL
    """
    suffix = path.suffix.lower()
    if suffix in (".jpg", ".jpeg"):
        mime = "image/jpeg"
    elif suffix == ".png":
        mime = "image/png"
    else:
        mime = "application/octet-stream"

    with path.open("rb") as f:
        data = f.read()
    return make_data_url_from_bytes(data, mime)


def load_pdf_first_page_to_data_url(path: Path) -> tuple[int, str]:
    """
    æ‰“å¼€ PDFï¼Œå–ç¬¬ä¸€é¡µï¼Œæ¸²æŸ“ä¸º PNGï¼Œå†è½¬ä¸º data URLã€‚
    è¿”å› (page_number, data_url)ï¼Œpage_number ä» 1 å¼€å§‹ã€‚
    å‡è®¾ PDF éƒ½æ˜¯å•é¡µï¼Œä½†ä»£ç æ”¯æŒæœ‰å¤šé¡µçš„æƒ…å†µï¼ˆè¿™é‡Œåªå–ç¬¬ä¸€é¡µï¼‰ã€‚
    """
    doc = fitz.open(path)
    if len(doc) == 0:
        raise ValueError(f"PDF æ–‡ä»¶æ²¡æœ‰é¡µé¢: {path}")
    page = doc[0]
    pix = page.get_pixmap()  # é»˜è®¤åˆ†è¾¨ç‡
    png_bytes = pix.tobytes("png")
    data_url = make_data_url_from_bytes(png_bytes, "image/png")
    return 1, data_url  # é¡µç ä» 1 å¼€å§‹


def extract_json(text: str) -> dict:
    """
    å¼ºå¥ JSON è§£æå™¨ï¼š
    - è‡ªåŠ¨æå– ```json ... ``` å—
    - è¯†åˆ«æœ€å¤–å±‚ { ... }
    - è‡ªåŠ¨ä¿®å¤ä¸­æ–‡ç¬¦å·ã€å•å¼•å·ã€æœªåŠ å¼•å·çš„ keyã€æœ«å°¾é€—å·
    - å°è¯•ä½¿ç”¨æ­£åˆ™ç»™ key è¡¥åŒå¼•å·
    """

    if not text:
        raise ValueError("Model output is empty")

    # 1. æ‰¾ ```json ... ``` éƒ¨åˆ†
    m = re.search(r"```json(.*?)```", text, re.S)
    if m:
        text = m.group(1)

    # 2. åªå–æœ€å¤–å±‚ {...}
    start = text.find("{")
    end = text.rfind("}")
    if start == -1 or end == -1:
        raise ValueError("No JSON object found in model output.")

    json_str = text[start:end+1]

    # ---------- è‡ªåŠ¨ä¿®å¤å¸¸è§æ¨¡å‹è¾“å‡ºé—®é¢˜ ----------

    # ï¼ˆAï¼‰æ›¿æ¢ä¸­æ–‡å†’å·ã€é€—å·
    json_str = json_str.replace("ï¼š", ":").replace("ï¼Œ", ",")

    # ï¼ˆBï¼‰ç»Ÿä¸€æ›¿æ¢å•å¼•å· -> åŒå¼•å·  
    # é˜²æ­¢æŠŠå€¼é‡Œçš„æ’‡å·ä¹Ÿæ›¿æ¢ï¼Œä½†ä¸€èˆ¬æ¨¡å‹ä¸ä¼šç”Ÿæˆè¿™ç§å¤æ‚æƒ…å†µ
    json_str = re.sub(r"'([^']*)'", r'"\1"', json_str)

    # ï¼ˆCï¼‰ç»™æœªåŠ å¼•å·çš„ key è‡ªåŠ¨è¡¥å¼•å·ï¼š  
    # ä¾‹å¦‚ï¼šamount: 100  ->  "amount": 100
    json_str = re.sub(
        r'(?m)^\s*([A-Za-z0-9_\u4e00-\u9fa5]+)\s*:',
        r'"\1":',
        json_str
    )

    # (D) ä¿®å¤è¡Œå†…æœªåŠ å¼•å· keyï¼Œä¾‹å¦‚ { amount: 100 }
    json_str = re.sub(
        r'({|,)\s*([A-Za-z0-9_\u4e00-\u9fa5]+)\s*:',
        r'\1 "\2":',
        json_str
    )

    # ï¼ˆEï¼‰å»æ‰å¤šä½™çš„é€—å·ï¼Œå¦‚ï¼š { "a":1, }
    json_str = re.sub(r",\s*([}\]])", r"\1", json_str)

    # ï¼ˆFï¼‰åˆ é™¤æ³¨é‡Šæˆ–å°¾éƒ¨åƒåœ¾
    json_str = re.sub(r"//.*?$", "", json_str, flags=re.M)
    json_str = re.sub(r"/\*.*?\*/", "", json_str, flags=re.S)

    # ---------- å°è¯•è§£æ ----------
    try:
        return json.loads(json_str)
    except Exception as e:
        print("========== RAW JSON STR ==========")
        print(json_str)
        print("==================================")
        raise ValueError(f"JSON parsing failed: {e}")



# =========================
# æ ¸å¿ƒå¤„ç†é€»è¾‘
# =========================
def build_doc_items() -> list[dict]:
    """
    å°† data ç›®å½•ä¸‹çš„æ–‡ä»¶è½¬æˆå¯ä¾›è°ƒç”¨æ¨¡å‹çš„ä»»åŠ¡åˆ—è¡¨ï¼š
    æ¯ä¸ªå…ƒç´ åŒ…å«ï¼š
    {
        "file": Path,
        "page": int,       # å¯¹å›¾ç‰‡å°±æ˜¯ 1ï¼Œå¯¹ PDF æ˜¯é¡µç ï¼ˆè¿™é‡Œåªå– 1ï¼‰
        "data_url": str,   # base64 data URL
    }
    """
    items: list[dict] = []
    for path in iter_docs(DATA_DIR):
        suffix = path.suffix.lower()
        if suffix in (".jpg", ".jpeg", ".png"):
            data_url = encode_image_file_to_data_url(path)
            items.append({"file": path, "page": 1, "data_url": data_url})
        elif suffix == ".pdf":
            # é»˜è®¤å•é¡µ PDFï¼Œè¿™é‡Œåªå–ç¬¬ä¸€é¡µ
            page_num, data_url = load_pdf_first_page_to_data_url(path)
            items.append({"file": path, "page": page_num, "data_url": data_url})
        else:
            # æ­£å¸¸ä¸ä¼šèµ°åˆ°è¿™é‡Œï¼Œå› ä¸ºå‰é¢å·²ç»è¿‡æ»¤
            continue

    if not items:
        raise RuntimeError(f"åœ¨ {DATA_DIR} ä¸‹æ²¡æœ‰æ‰¾åˆ° jpg/jpeg/png/pdf æ–‡ä»¶ã€‚")
    return items


def call_model_on_doc(api_key, model_name, prompt, file, page, data_url):
    # print("THREAD ENV:", os.getenv("OPENAI_API_KEY"))

    # æ¯ä¸ªçº¿ç¨‹ç‹¬ç«‹ client 
    client = OpenAI(
        api_key=api_key,
        base_url=BASE_URL
    )

    messages = [
        {
            "role": "user",
            "content": [
                {"type": "text", "text": prompt},
                {"type": "image_url", "image_url": {"url": data_url}},
            ],
        }
    ]

    start_ts = time.time()
    completion = client.chat.completions.create(
        model=model_name,
        messages=messages,
        max_tokens=800,
    )
    elapsed = time.time() - start_ts

    # content æå–å’Œ JSON 
    content = completion.choices[0].message.content

    # DashScope æœ‰æ—¶ content å¯èƒ½æ˜¯ listï¼Œè¿™é‡Œåšå…¼å®¹
    if isinstance(content, list):
        text_parts = []
        for part in content:
            if isinstance(part, dict) and part.get("type") == "text":
                text_parts.append(part.get("text", ""))
        output_text = "".join(text_parts)
    else:
        output_text = content or ""
        # print("=== RAW OUTPUT ===")
        # print(output_text)
        # print("==================")


    parsed = extract_json(output_text)

    return model_name, file, page, parsed, elapsed


def plot_per_file_timing_multi_round(results_by_model, out_path):
    import matplotlib.pyplot as plt
    import numpy as np
    import os
    import matplotlib.pyplot as plt
    import matplotlib

    # Set Chinese font
    matplotlib.rcParams['font.sans-serif'] = ['Microsoft YaHei']
    matplotlib.rcParams['axes.unicode_minus'] = False

    # --- æ”¶é›†æ‰€æœ‰æ–‡ä»¶åï¼ˆä¿æŒé¡ºåºï¼‰ ---
    all_files = []
    for model in results_by_model.values():
        for case in model["cases"]:
            fname = os.path.basename(case["file"])
            if fname not in all_files:
                all_files.append(fname)

    # --- ç»˜å›¾ ---
    plt.figure(figsize=(16, 6))

    for model_name, model_data in results_by_model.items():
        # è®°å½•æ¨¡å‹å¯¹æ¯ä¸ªæ–‡ä»¶çš„å¤šè½®è€—æ—¶
        file_time_map = {}  # fname â†’ list[time_taken]

        for case in model_data["cases"]:
            fname = os.path.basename(case["file"])
            t = case.get("time_taken", 0)
            file_time_map.setdefault(fname, []).append(t)

        # è®¡ç®—å¹³å‡å€¼ï¼ˆ10 è½®ï¼‰
        avg_times = []
        for f in all_files:
            if f in file_time_map:
                avg_times.append(np.mean(file_time_map[f]))
            else:
                avg_times.append(0)

        plt.plot(all_files, avg_times, marker="o", label=model_name)

    plt.xticks(rotation=45, ha="right")
    plt.ylabel("Average Time Taken (s)")
    plt.title("Per-File Timing Comparison (Averaged Over Multiple Rounds)")
    plt.legend()
    plt.tight_layout()
    plt.savefig(out_path, dpi=200)
    plt.close()


def main():
    # è¯»å– API Key
    api_key = os.getenv(API_KEY_ENV)
    if not api_key:
        raise RuntimeError("æœªæ£€æµ‹åˆ°ç¯å¢ƒå˜é‡ OPENAI_API_KEY")

    # è¯»å–ç»Ÿä¸€ prompt
    prompt = load_prompt()

    # æ„å»ºæ–‡æ¡£ä»»åŠ¡åˆ—è¡¨ï¼ˆæ”¯æŒ jpg/jpeg/png/pdfï¼‰
    doc_items = build_doc_items()

    task_start_iso = now_iso()

    # ç»“æœç»“æ„ï¼šä¸åŒ…å«ä»»ä½•æ—¶é—´å­—æ®µï¼ˆæŒ‰ä½ çš„è¦æ±‚ï¼‰
    results_by_model: dict[str, dict] = {
        model: {"model": model, "cases": []} for model in MODELS
    }

    # ç”¨äºç»Ÿè®¡å¹³å‡è€—æ—¶ï¼ˆä¸å†™å…¥ JSONï¼Œåªç”¨äºç»˜å›¾ï¼‰
    time_records: dict[str, list[float]] = {model: [] for model in MODELS}

    # å¹¶è¡Œæ‰§è¡Œï¼šå¯¹æ¯ä¸ªæ¨¡å‹ Ã— æ¯ä¸ª doc_item æ„å»ºä»»åŠ¡
    futures = []
    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
        for model in MODELS:
            for item in doc_items:
                future = executor.submit(
                    call_model_on_doc,
                    api_key,
                    model,
                    prompt,
                    item["file"],
                    item["page"],
                    item["data_url"],
                )
                future.model_name = model   # â­ç»‘å®šæ¨¡å‹å
                futures.append(future)

        for fut in as_completed(futures):
            try:
                model_name, file, page, parsed, elapsed = fut.result()
            except Exception as e:
                print(f"[ERROR] æ¨¡å‹ {fut.model_name} è°ƒç”¨å¼‚å¸¸: {e}")  # â­æ‰“å°æ¨¡å‹å
                continue


            if "cases" not in results_by_model[model_name]:
                results_by_model[model_name]["cases"] = []

            results_by_model[model_name]["cases"].append(
                {
                    "file": str(file),
                    "page": page,
                    "output": parsed,
                    "time_taken": elapsed,   # â­ ä¿å­˜æ¯æ–‡ä»¶è€—æ—¶
                }
            )

    task_end_iso = now_iso()

    # ç»„ç»‡æœ€ç»ˆ JSONï¼ˆä¸åŒ…å«ä»»ä½•æ—¶é—´å­—æ®µï¼‰
    all_results = {
        "task_start": task_start_iso,
        "task_end": task_end_iso,
        "models": list(results_by_model.values()),
    }

    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)
    with OUTPUT_JSON.open("w", encoding="utf-8") as f:
        json.dump(all_results, f, ensure_ascii=False, indent=2)

    # è®¡ç®—æ¯ä¸ªæ¨¡å‹çš„å¹³å‡è€—æ—¶
    avg_time = {}
    for model, times in time_records.items():
        if times:
            avg_time[model] = sum(times) / len(times)
        else:
            avg_time[model] = 0.0

    # ç”»å›¾
    PLOT_MULTI = OUTPUT_DIR / "per_file_timing_multi_round.png"
    plot_per_file_timing_multi_round(results_by_model, PLOT_MULTI)
    print(f"ğŸ“Š å¤šè½®å¹³å‡è€—æ—¶å›¾: {PLOT_MULTI}")



if __name__ == "__main__":
    main()
