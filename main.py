import asyncio
from pathlib import Path
import json
from datetime import datetime
import time

from src.pre_processor import preprocess_file
from src.run_model import run_one_file
from src.rulebased_classifier import (
    run_ocr_async,
    rule_classify,
)
from typing import List

RAW_DIR = Path("data/raw")
PROCESSED_DIR = Path("data/processed")
PROMPT_DIR = Path("prompts")
OUTPUT_DIR = Path("outputs")

PROMPT_MAP = {
    "itinerary": PROMPT_DIR / "itinerary_prompt.txt",
    "hotel_invoice": PROMPT_DIR / "hotel_prompt.txt",
    "payment": PROMPT_DIR / "payment_prompt.txt",
    "other": PROMPT_DIR / "other_prompt.txt",
}



async def batch_ocr_and_classify(docs):
    """
    docs: List[List[Path]]
      - å¤–å±‚ï¼šæ¯ä¸ªå…ƒç´ æ˜¯ä¸€ä»½æ–‡æ¡£
      - å†…å±‚ï¼šè¯¥æ–‡æ¡£çš„æ‰€æœ‰é¡µé¢ jpg è·¯å¾„

    è¿”å›ï¼šæ¯ä¸ªæ–‡æ¡£ä¸€æ¡ç»“æœï¼š
      {
        "doc_id": ...,
        "pages": [...],
        "type": ...,
      }
    """

    async def classify_one_doc(pages):
        # ä¿æŠ¤ä¸€ä¸‹ï¼Œé¿å…ç©ºåˆ—è¡¨
        if not pages:
            return {
                "doc_id": None,
                "pages": [],
                "type": "other",
            }

        first_page = pages[0]

        # 1. ç”¨ç¬¬ä¸€é¡µåš OCR
        text = await run_ocr_async(first_page)

        # 2. ç”¨ OCR æ–‡æœ¬åšè§„åˆ™åˆ†ç±»
        doc_type = await rule_classify(text)

        # 3. ä»ç¬¬ä¸€é¡µçš„æ–‡ä»¶åé‡Œæ¨ä¸€ä¸ª doc_id
        stem = first_page.stem  # æ¯”å¦‚ "trip_page1" æˆ– "hotel"
        if "_page" in stem:
            doc_id = stem.rsplit("_page", 1)[0]  # "trip_page1" -> "trip"
        else:
            doc_id = stem

        return {
            "doc_id": doc_id,
            "pages": pages,
            "type": doc_type,
        }

    # æ‰€æœ‰æ–‡æ¡£å¹¶å‘åˆ†ç±»
    tasks = [classify_one_doc(pages) for pages in docs]
    results = await asyncio.gather(*tasks)
    return list(results)


async def extract_one(pages: List[Path], doc_type: str):
    """
    å¯¹â€œåŒä¸€ä»½æ–‡æ¡£â€çš„æ‰€æœ‰é¡µé¢è¿›è¡ŒæŠ½å–ï¼š
    - pages: è¿™ä¸€ä»½æ–‡æ¡£çš„æ‰€æœ‰é¡µé¢ jpg è·¯å¾„ï¼ˆè‡³å°‘æœ‰ 1 ä¸ªï¼‰
    - doc_type: æ–‡æ¡£ç±»å‹ï¼ˆè¡Œç¨‹å• / é…’åº—æ°´å• / æ”¯ä»˜è®°å½• ç­‰ï¼‰

    å½“å‰å®ç°ï¼šæŠŠæ•´ä»½æ–‡æ¡£çš„æ‰€æœ‰ pages åˆ—è¡¨ç›´æ¥ä¼ ç»™ run_one_fileï¼Œ
    åç»­ä½ ä¼šåœ¨ run_one_file å†…éƒ¨å®ç°â€œå¤šé¡µåˆå¹¶ + è°ƒç”¨ LLMâ€çš„é€»è¾‘ã€‚
    """
    if doc_type not in PROMPT_MAP:
        return None

    if not pages:
        return None

    prompt_path = PROMPT_MAP[doc_type]

    # ä»£è¡¨è¿™ä¸€ä»½æ–‡æ¡£çš„â€œä¸»æ–‡ä»¶åâ€ï¼Œç”¨ç¬¬ä¸€é¡µçš„åå­—å³å¯
    first_page = pages[0]

    # å…³é”®ç‚¹ï¼šè¿™é‡ŒæŠŠã€Œæ•´ä»½æ–‡æ¡£çš„æ‰€æœ‰é¡µé¢ã€ä¼ ç»™ run_one_file
    # ä½ åé¢ä¼šæŠŠ run_one_file æ”¹æˆå¯ä»¥æ¥æ”¶ List[Path] å¹¶æ„é€ å¤šé¡µè¾“å…¥
    result = await run_one_file(pages, prompt_path)

    # Try parsing JSON
    try:
        output = result.get("output")
        if isinstance(output, str):
            result["output"] = json.loads(output)
    except Exception:
        pass

    return {
        # å…¼å®¹æ—§å­—æ®µï¼Œç”¨ç¬¬ä¸€é¡µåå­—åšâ€œä»£è¡¨æ–‡ä»¶åâ€
        "processed_file": first_page.name,
        # é¢å¤–è¿”å›è¿™ä¸€ä»½æ–‡æ¡£çš„æ‰€æœ‰é¡µé¢åï¼Œæ–¹ä¾¿è°ƒè¯• / è¿½æº¯
        "all_pages": [p.name for p in pages],
        "type": doc_type,
        "result": result,
    }



async def main():
    OUTPUT_DIR.mkdir(exist_ok=True)

    # ---- Load and preprocess files ----
    raw_files = sorted(RAW_DIR.iterdir())

    # è¿™é‡Œæ¯ä¸ª preprocess_file è¿”å› List[Path]ï¼ˆä¸€ä»½æ–‡æ¡£çš„æ‰€æœ‰é¡µé¢ï¼‰
    processed_docs = [preprocess_file(f, PROCESSED_DIR) for f in raw_files]
    # processed_docs: List[List[Path]]

    # ---- Batch OCR + classification (doc-level) ----
    t0 = time.time()
    print("\nğŸ” Running batch OCR + classification ...")
    batch_results = await batch_ocr_and_classify(processed_docs)
    t1 = time.time()
    print(f"ğŸ•’ Classification took: {t1 - t0:.2f} seconds")
    # batch_results é‡Œæ¯ä¸ª item:
    # {
    #   "doc_id": ...,
    #   "pages": [...],
    #   "type": ...,
    # }

    # ---- Extraction tasks ----
    extract_tasks = []
    for item in batch_results:
        doc_type = item["type"]
        if doc_type not in PROMPT_MAP:
            print(f"âŒ Unknown type: {doc_type}, skipping doc {item['doc_id']}")
            continue

        pages = item["pages"]  # List[Path]ï¼Œè¿™ä¸€ä»½æ–‡æ¡£çš„æ‰€æœ‰é¡µé¢
        # å»ºè®®æŠŠ extract_one æ”¹æˆæŒ‰â€œæ–‡æ¡£çº§â€æ¥æŠ½å–ï¼š
        # async def extract_one(pages: List[Path], doc_type: str): ...
        extract_tasks.append(extract_one(pages, doc_type))
    
    t2 = time.time()
    # Run all extraction in parallel
    extracted = await asyncio.gather(*extract_tasks)
    t3 = time.time()
    print(f"ğŸ•’ Extraction (LLM) took: {t3 - t2:.2f} seconds")
    return [e for e in extracted if e is not None]



if __name__ == "__main__":
    start_time = time.time()

    results = asyncio.run(main())

    total_time = time.time() - start_time
    print(f"\nâ± Total time taken: {total_time:.2f} seconds")

    now = datetime.now().strftime("%m%d%H%M")
    out_file = OUTPUT_DIR / f"output_{now}.json"

    output_json = {
        "total_time_seconds": round(total_time, 2),
        "results": results
    }

    out_file.write_text(
        json.dumps(output_json, ensure_ascii=False, indent=2),
        encoding="utf-8"
    )

    print("\nğŸ‰ ALL DONE")
    print(f"ğŸ“„ Output saved to: {out_file}")
